
import eventlet
eventlet.monkey_patch()

from flask import Flask, request, jsonify
from flask_socketio import SocketIO, emit
import io
import base64
from PIL import Image, ImageDraw, ImageFont 
import requests
from datetime import datetime
from zoneinfo import ZoneInfo
import time
import cv2
import boto3          # NCP Object Storageë¥¼ ì‚¬ìš©í•˜ê³  ì‹¶ì€ ê²½ìš°, ë°˜ë“œì‹œ ì„¤ì¹˜
from ultralytics import YOLO
import os
os.environ.setdefault("TORCH_CPP_LOG_LEVEL", "ERROR")
os.environ.setdefault("GLOG_minloglevel", "2")
import torch
import numpy as np
from typing import Union
import os
from eventlet.greenpool import GreenPool


# =================================================================
#                         0.ì‚¬ì „ ì„¤ì •
# =================================================================

# [ Object Storage ì—…ë¡œë“œë¥¼ ìœ„í•œ ì‚¬ì „ ì„¤ì • ]
# Python 3.13 ë²„ì „ê³¼ boto3 ìµœì‹  ë²„ì „ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì‚¬ì „ ì„¤ì •
os.environ.setdefault("AWS_REQUEST_CHECKSUM_CALCULATION", "when_required")
os.environ.setdefault("AWS_RESPONSE_CHECKSUM_VALIDATION", "when_required")


# [ numpy â†’ íŒŒì´ì¬ íƒ€ì… ë³€í™˜ ì„¤ì • ]
def _to_py(obj):
    import numpy as np
    if isinstance(obj, (np.bool_,)):
        return bool(obj)
    if isinstance(obj, (np.integer,)):
        return int(obj)
    if isinstance(obj, (np.floating,)):
        return float(obj)
    if isinstance(obj, (np.ndarray,)):
        return obj.tolist()
    if isinstance(obj, dict):
        return {k: _to_py(v) for k, v in obj.items()}
    if isinstance(obj, (list, tuple)):
        return [_to_py(v) for v in obj]
    return obj


# [ Ultralytics ëª¨ë¸ í´ë˜ìŠ¤ ì‹ ë¢° ]
# PyTorchì˜ ë³´ì•ˆ ë¡œë”© ì •ì±…ì— Ultralytics ëª¨ë¸ í´ë˜ìŠ¤ë¥¼ ì‹ ë¢°í•  ìˆ˜ ìˆë„ë¡ ì¶”ê°€í•©ë‹ˆë‹¤.
# ì´ ì½”ë“œëŠ” YOLO ëª¨ë¸ì„ ë¡œë”©í•˜ê¸° *ì „ì—* ì‹¤í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
try:
    # YOLO ëª¨ë¸ ì•„í‚¤í…ì²˜ë¥¼ êµ¬ì„±í•˜ëŠ” ë° í•„ìš”í•œ ê±°ì˜ ëª¨ë“  í´ë˜ìŠ¤ë¥¼ import í•©ë‹ˆë‹¤.
    from torch.nn.modules.container import Sequential
    from ultralytics.nn.tasks import DetectionModel
    from ultralytics.nn.modules import (
        Conv, C2f, Bottleneck, Concat, SPPF, Detect
    )
    
    # ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í´ë˜ìŠ¤ ëª©ë¡ ì „ì²´ë¥¼ í•œ ë²ˆì— ì¶”ê°€í•©ë‹ˆë‹¤.
    torch.serialization.add_safe_globals([
        DetectionModel, Sequential, Conv, C2f, 
        Bottleneck, Concat, SPPF, Detect
    ])
    print("âœ… PyTorch safe globalsì— YOLO í•„ìˆ˜ ëª¨ë¸ í´ë˜ìŠ¤ë“¤ ì¶”ê°€ ì„±ê³µ!")
except Exception as e:
    print(f"âš ï¸ PyTorch safe globals ì„¤ì • ì¤‘ ê²½ê³  ë°œìƒ: {e}")


# [ ì˜ìƒ ì „ì†¡ íŒŒë¼ë¯¸í„° ]
# ì‹¤íš¨ ì „ì†¡ ìƒí•œ (ì„œë²„ì¸¡ ì“°ë¡œí‹€)         0.20s ~= 5fps, 0.125s ~= 8fps
MIN_INTERVAL = float(os.getenv("AI_MIN_INTERVAL", "0.05"))

# ì„œë²„ê°€ ê¶Œì¥í•˜ëŠ” ë¯¸ë¦¬ë³´ê¸° FPS (í”„ë¡ íŠ¸ê°€ ë°›ìœ¼ë©´ ê°„ê²©ì„ ë§ì¶”ê²Œ ë¨)
SERVER_TARGET_FPS = float(os.getenv("SERVER_TARGET_FPS", "20.0"))
AI_PARALLEL = int(os.getenv("AI_PARALLEL", "3"))    # ë™ì‹œ ì‹¤í–‰ ëª¨ë¸ ìˆ˜(ìµœëŒ€ 3)

# ê³µìš© ê·¸ë¦°í’€ (ìš”ì²­ë§ˆë‹¤ ìƒˆë¡œ ë§Œë“¤ì§€ ì•Šê³  ì¬ì‚¬ìš©)
GREEN_POOL = GreenPool(size=AI_PARALLEL)


# =================================================================
#               1. Flask ì•± ë° SocketIO ì„œë²„ ì´ˆê¸°í™”
# =================================================================

app = Flask(__name__)
# cors_allowed_origins="*" : React ê°œë°œ ì„œë²„ ë“± ëª¨ë“  ì£¼ì†Œì—ì„œì˜ ì—°ê²°ì„ í—ˆìš©
# socketio = SocketIO(app, cors_allowed_origins="*", async_mode="eventlet")/
socketio = SocketIO(
    app,
    cors_allowed_origins="*",
    async_mode="eventlet",
    ping_interval=25,
    ping_timeout=60,
    max_http_buffer_size=8 * 1024 * 1024,
)

# =================================================================
#               1-1. ì „ì—­ ìƒíƒœ ê´€ë¦¬ ë³€ìˆ˜ (ìˆ˜ì •) 
# =================================================================

# [ í˜„ì¬ ë©”ì¸ í™”ë©´ìœ¼ë¡œ ì„ íƒëœ ì¥ë¹„ì˜ ID ]
main_device_id = None

# [ í•˜ë‹¨ ë²„íŠ¼ì— ëŒ€í•œ ì „ì—­ íƒì§€ ê¸°ëŠ¥ On/Off ìƒíƒœ ]
global_toggle_states = {
    'ppe': False,
    'acc': False,
    'he': False
}

# [ OCR ì¬ì‹œë„ íšŸìˆ˜ ê¸°ë¡ì„ ìœ„í•œ ì „ì—­ ë³€ìˆ˜ ë° ìƒìˆ˜ ì¶”ê°€ ]
heavy_equipment_ocr_attempts = {}
OCR_RETRY_THRESHOLD = 1                 # ë™ì¼ IDì— ëŒ€í•œ OCR ì‹¤íŒ¨ í—ˆìš© íšŸìˆ˜
last_ocr_call_ts = 0.0                  # OCR í˜¸ì¶œ ìµœí›„ ì‹œê°
OCR_COOLDOWN_SEC = 0.8                  # OCR ìµœì†Œ í˜¸ì¶œ ê°„ê²©(ì´ˆ) (ìƒí™©ì— ë”°ë¼ 0.5 - 2.0ë¡œ ì¡°ì •)
FRAME_STRIDE_FOR_OCR = 3                # ní”„ë ˆì„ë§ˆë‹¤ë§Œ OCR ì‹œë„
ocr_backoff_until = 0.0                 # ì‹¤íŒ¨ ì‹œ ë°±ì˜¤í”„ ì¢…ë£Œ ì‹œê° (time.time() ê¸°ì¤€)
he_frame_counter = 0                    # detect_he() í˜¸ì¶œ ì¹´ìš´í„°


# [ ë™ì‹œ ì²˜ë¦¬ ë³€ìˆ˜ ]
processing_inflight = set()
last_frame_ts = {}


# [ ì²« í™”ë©´ ì˜ìƒ ìŠ¤íŠ¸ë¦¼ ìƒíƒœ ]
current_video_stream = None
thread_for_video_processing = None
current_stream_owner_sid = None 


# [ ì˜¤ë˜ëœ í‚¤ ë©”ëª¨ë¦¬ ì£¼ê¸°ì  ì •ë¦¬ ]
CLEANUP_INTERVAL_SEC = 60      # ì„¤ì •ê°’ (ì´ˆ) ë§ˆë‹¤ ì •ë¦¬
TRACK_TTL_SEC        = 300     # ì„¤ì •ê°’ (ì´ˆ) ì§€ë‚˜ë©´ íŠ¸ë™ ê´€ë ¨ ìºì‹œ ì œê±°
COOLDOWN_TTL_SEC     = 900     # ì„¤ì •ê°’ (ì´ˆ) ì§€ë‚˜ë©´ ê¸°ë¡ ì¿¨ë‹¤ìš´ í‚¤ ì œê±°
FRAME_TS_TTL_SEC     = 600     # ì„¤ì •ê°’ (ì´ˆ) ì§€ë‚œ last_frame_ts ì œê±°

# [ ì¤‘ì¥ë¹„ OCR ì¬ì‹œë„/í™œë™ ì¶”ì ìš© ]
# stale ì œê±° ìœ„í•´ ë§ˆì§€ë§‰ ë³¸ ì‹œê° ê¸°ë¡
he_last_seen = {}


# [ í•œê¸€ í°íŠ¸ ìë™ íƒìƒ‰ + ìºì‹± ]
_SELECTED_FONT = None
_FONT_CACHE = {}

def load_korean_font(size=24):
    if size in _FONT_CACHE:
        return _FONT_CACHE[size]
    
    global _SELECTED_FONT
    candidates = [
        # ì‚¬ì „ì— ì„¤ì¹˜ëœ ë‚˜ëˆ” í°íŠ¸ë¥¼ ê°€ì ¸ì™€ì„œ ì‚¬ìš©
        os.path.join("fonts", "NanumGothic.ttf"),
        os.path.join("fonts", "NanumGothicBold.ttf"),
        "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
        "/usr/share/fonts/truetype/nanum/NanumGothicBold.ttf",
        "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",
        "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",
    ]
    for path in candidates:
        if os.path.exists(path):
            try:
                font = ImageFont.truetype(path, size=size, index=0)
                _SELECTED_FONT = path
                _FONT_CACHE[size] = font
                print(f"[font] using: {path}")
                return font
            except Exception:
                pass
    print("[font] fallback: default")
    # ìµœí›„ í´ë°± (ì˜ë¬¸ ì „ìš©ì¸ ê²½ìš°)
    font = ImageFont.load_default()
    _FONT_CACHE[size] = font
    return font


# [ ë°”ìš´ë”© ë°•ìŠ¤ í‘œì‹œ ì¢…ë¥˜ ì„¤ì • ]
SHOW_PPE_GEAR_BOXES = int(os.getenv("SHOW_PPE_GEAR_BOXES", "0")) # ê¸°ë³¸ off


# =================================================================
#                 2. íƒì§€ìœ í˜•ë³„ AI ëª¨ë¸ í•¨ìˆ˜ í˜¸ì¶œ
# =================================================================

# [ AI ëª¨ë¸ ë¡œë”© ]  
print("AI ëª¨ë¸ì„ ë¡œë”©í•©ë‹ˆë‹¤...")             # (ì„œë²„ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰)
if torch.cuda.is_available():
    print("CUDA is available! Using GPU.")
    device = "cuda"
else:
    print("CUDA is not available. Using CPU.")
    device = "cpu"

try:
    # ì•ˆì „ì¥ë¹„ ì°©ìš©ì—¬ë¶€ íƒì§€ ëª¨ë¸ 
    ppe_model = YOLO("models/ppe_model(11s + 50).pt").to(device)        # ìµœì¢…ì ìœ¼ë¡œ ppe_model.pt ë¡œ ì´ë¦„ ìˆ˜ì •
    # ì‚¬ê³ ê°ì§€ ëª¨ë¸
    acc_model = YOLO("models/yolo11s-pose.pt").to(device)               # ìµœì¢…ì ìœ¼ë¡œ acc_model.pt ë¡œ ì´ë¦„ ìˆ˜ì •
    # ì¤‘ì¥ë¹„ íƒì§€ ëª¨ë¸
    he_model = YOLO("models/he_model(11s + 40).pt").to(device)          # ìµœì¢…ì ìœ¼ë¡œ he_model.pt ë¡œ ì´ë¦„ ìˆ˜ì •
        # ì¤‘ì¥ë¹„ íƒì§€ ëª¨ë¸
    base_model = YOLO("models/yolo11s-track.pt").to(device)             # ìµœì¢…ì ìœ¼ë¡œ base_model.pt ë¡œ ì´ë¦„ ìˆ˜ì •

    print("âœ… AI ëª¨ë¸ ë¡œë”© ì„±ê³µ!")
except Exception as e:
    print(f"âŒ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    exit()


# -----------------------------------
#        ì•ˆì „ì¥ë¹„ ì°©ìš©ì—¬ë¶€ AI
# -----------------------------------

# PPEì—ì„œ ì‚¬ëŒ íŠ¸ë™ìš© ëª¨ë¸ ì„ ì •
# ê¸°ë³¸ê°’ìœ¼ë¡œ YOLOv11 ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©í•¨
USE_BASE_FOR_PPE = int(os.getenv("USE_BASE_FOR_PPE", "1"))  # 1=on, 0=off

# ì‚¬ëŒ íƒì§€ ì„¤ì •
PERSON_LABEL = 'person'

# ì•ˆì „ì¥ë¹„ ì°©ìš©ì—¬ë¶€ ì„¤ì •
SAFETY_LABELS = {
    'wear': ['Safety Helmet wear', 'Safety Belt wear', 'Safety Hook wear', 'Safety Shoes wear'],
    'unwear': ['Safety Helmet unwear', 'Safety Belt unwear', 'Safety Hook unwear', 'Safety Shoes unwear']
}
PERSON_BBOX_EXPANSION_FACTOR = 0.10


# [ ì•ˆì „ì¥ë¹„ ì°©ìš© ì—¬ë¶€ íƒì§€ í•¨ìˆ˜ ]
def detect_ppe(image_bytes):
    # PIL Imageë¥¼ OpenCV í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
    image = Image.open(io.BytesIO(image_bytes))
    frame = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)

    detections = []
    violations_to_record = []
    

    # 1. ì‚¬ëŒ ê°ì²´ íƒì§€ 
    # ì‚¬ëŒì´ë¼ëŠ” ê°ì²´ë¥¼ íƒì§€í•˜ê¸° ìœ„í•´ ê¸°ë³¸ ëª¨ë¸ì„ ì‚¬ìš©í•¨ (ì„¤ì •ê°€ëŠ¥)
    if USE_BASE_FOR_PPE and 'base_model' in globals() and base_model is not None:
        person_results = base_model.track(
            frame, persist=True, conf=0.25, iou=0.5, max_det=50, verbose=False, classes=[0] # ì‚¬ëŒ ê°ì²´
        )
    else:
        person_results = acc_model.track(
            frame, persist=True, conf=0.25, iou=0.5, max_det=50, verbose=False
        )

    person_detections = []
    for r in person_results:
        if r.boxes.id is not None:
            for box in r.boxes:
                cls = int(box.cls[0])
                class_name = r.names[cls]
                if class_name == PERSON_LABEL:
                    x1, y1, x2, y2 = map(int, box.xyxy[0])
                    conf = float(box.conf[0])
                    track_id = int(box.id.cpu().numpy().item())
                    person_detections.append({
                        'class_name': class_name, 
                        'confidence': conf, 
                        'bbox': (x1, y1, x2, y2), 
                        'track_id': track_id
                    })
                    detections.append({
                        'box': [x1, y1, x2, y2], 
                        'label': f"{class_name} (ID: {track_id})", 
                        'confidence': conf, 
                        'track_id': track_id,
                        'source': 'ppe',
                        'kind': 'person'
                    })
    

    # 2. ì•ˆì „ ì¥ë¹„ ìœ„ë°˜ íƒì§€ 
    # ì•ˆì „ì¥ë¹„ ì°©ìš©ì—¬ë¶€ë¥¼ íƒì§€í•˜ê¸° ìœ„í•´ ppe_model ì‚¬ìš©í•¨
    safety_gear_results = ppe_model(frame, stream=True)

    safety_gear_detections = []
    gear_out = []

    def _parse_gear(name: str):
        n = name.lower()
        part = "unknown"
        if "helmet" in n: part = "helmet"
        elif "belt" in n: part = "belt"
        elif "hook" in n: part = "hook"
        elif "shoes" in n: part = "shoes"
        state = "wear" if "unwear" not in n else "unwear"
        return part, state

    for r in safety_gear_results:
        for box in r.boxes:
            cls = int(box.cls[0])
            class_name = r.names[cls]
            # ì°©ìš©/ë¯¸ì°©ìš© ëª¨ë“  ì•ˆì „ì¥ë¹„ ë¼ë²¨ì„ íƒì§€
            if class_name in SAFETY_LABELS['wear'] + SAFETY_LABELS['unwear']:
                x1, y1, x2, y2 = map(int, box.xyxy[0])
                conf = float(box.conf[0])
                safety_gear_detections.append({
                    'class_name': class_name, 
                    'confidence': conf, 
                    'bbox': (x1, y1, x2, y2)
                })


    # ì‚¬ëŒ ë°•ìŠ¤ì˜ í™•ì¥ì˜ì—­ ë¯¸ë¦¬ ê³„ì‚° (ì¥ë¹„-ì‚¬ëŒ ë§¤ì¹­ìš©)
    people_expanded = []
    for p in person_detections:
        px1, py1, px2, py2 = p['bbox']
        pw, ph = px2 - px1, py2 - py1
        ex = int(pw * PERSON_BBOX_EXPANSION_FACTOR / 2)
        ey = int(ph * PERSON_BBOX_EXPANSION_FACTOR / 2)
        exp = (
            max(0, px1 - ex), max(0, py1 - ey),
            min(frame.shape[1], px2 + ex), min(frame.shape[0], py2 + ey)
        )
        people_expanded.append({**p, 'exp_bbox': exp})

    # ì¥ë¹„ ë°•ìŠ¤ë¥¼ ê°€ì¥ ê°€ê¹Œìš´ (í˜¹ì€ í¬í•¨í•˜ëŠ”) ì‚¬ëŒ íŠ¸ë™ì— ì—°ê²°í•´ì„œ gear_out ìƒì„±
    for s in safety_gear_detections:
        sx1, sy1, sx2, sy2 = s['bbox']
        cx, cy = (sx1 + sx2) / 2.0, (sy1 + sy2) / 2.0
        parent_tid = None
        for pe in people_expanded:
            ex1, ey1, ex2, ey2 = pe['exp_bbox']
            if ex1 <= cx <= ex2 and ey1 <= cy <= ey2:
                parent_tid = pe['track_id']
                break

        part, state = _parse_gear(s['class_name'])
        gear_out.append({
            'box': [sx1, sy1, sx2, sy2],
            'label': s['class_name'],
            'confidence': s['confidence'],
            'kind': 'gear',                # í”„ë¡ íŠ¸ì—ì„œ ìƒ‰ êµ¬ë¶„ ê°€ëŠ¥
            'source': 'ppe_gear',
            'gear_part': part,             # helmet / belt / hook / shoes
            'gear_state': state,           # wear / unwear
            'parent_track_id': parent_tid  # ì—°ê²°ëœ ì‚¬ëŒ íŠ¸ë™ (ì—†ìœ¼ë©´ None)
        })

    final_detections = []
    violators = [] 

    # 3. ì‚¬ëŒ ë°”ìš´ë”© ë°•ìŠ¤ ë‚´ì— ìœ„ë°˜ ì‚¬í•­ì´ ìˆëŠ”ì§€ í™•ì¸
    for p_det in person_detections:
        px1, py1, px2, py2 = p_det['bbox']
        # ã…‡ ë°”ìš´ë”© ë°•ìŠ¤ í™•ì¥
        pw, ph = px2 - px1, py2 - py1
        ex, ey = int(pw * PERSON_BBOX_EXPANSION_FACTOR / 2), int(ph * PERSON_BBOX_EXPANSION_FACTOR / 2)
        px1_exp, py1_exp = max(0, px1 - ex), max(0, py1 - ey)
        px2_exp, py2_exp = min(frame.shape[1], px2 + ex), min(frame.shape[0], py2 + ey)

        # ã…‡ í™•ì¥ëœ ë°”ìš´ë”© ë°•ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ìœ„ë°˜ ì‚¬í•­ ì°¾ê¸°
        person_safety_labels = set()
        for s_det in safety_gear_detections:
            sx1, sy1, sx2, sy2 = s_det['bbox']
            if (sx1 >= px1_exp and sy1 >= py1_exp and sx2 <= px2_exp and sy2 <= py2_exp):
                person_safety_labels.add(s_det['class_name'])

        final_detections.append({
            'box': [px1, py1, px2, py2],
            'label': f"person (ID: {p_det['track_id']})",
            'confidence': p_det['confidence'],
            'safety_status': list(person_safety_labels),
            'track_id': p_det['track_id'],
            'source': 'ppe',
            'kind': 'person' 
        })

        if any(lbl in SAFETY_LABELS['unwear'] for lbl in person_safety_labels):
            violators.append((p_det['track_id'], person_safety_labels))

    # ë°˜í™˜ ì§ì „ ì¥ë¹„ ë°•ìŠ¤ë„ í¬í•¨í•´ì„œ í”„ë¡ íŠ¸ë¡œ ì „ë‹¬
    final_for_ui = final_detections + (gear_out if SHOW_PPE_GEAR_BOXES else [])


    # ê¸°ë¡ ì „ì†¡ìš© ìœ„ë°˜ ê°ì²´ë“¤ : ìµœì¢… ë°•ìŠ¤ ì„¸íŠ¸(final_for_front)ë¥¼ í•¨ê»˜ ë„˜ê²¨ ê·¸ë ¤ì§
    for tid, labels in violators:
        violations_to_record.append({
            'track_id': tid,
            'labels': labels,
            'image': image,
            'detections': final_detections + gear_out
        })

    return final_for_ui, violations_to_record


# -----------------------------------
#           ì‚¬ê³  ê°ì§€ AI
# -----------------------------------

# ë‚™ìƒ ê°ì§€ ì„¤ì •
# FALL_RATIO_THRESHOLD = 0.8
# FALL_COUNT_THRESHOLD = 5        # ë‚™ìƒìœ¼ë¡œ íŒë‹¨í•˜ê¸° ìœ„í•œ ì—°ì† í”„ë ˆì„ ìˆ˜
# fall_status = {}                # track_id ë³„ ë‚™ìƒ ì¹´ìš´íŠ¸ë¥¼ ì €ì¥


# [ COCO 17 Keypoints ì¸ë±ìŠ¤ ì •ì˜ ]
NOSE = 0
L_SHOULDER, R_SHOULDER = 5, 6
L_HIP, R_HIP = 11, 12
L_KNEE, R_KNEE = 13, 14
L_ANKLE, R_ANKLE = 15, 16

# [ ë‹¤ì¤‘ ê·œì¹™ + ê°€ì¤‘ì¹˜ ] (íŠœë‹ í¬ì¸íŠ¸)
CRITERIA = {
    "aspect_ratio":     {"weight": 1.0, "test": lambda ar: ar < 0.72},
    "vertical_sh_hip":  {"weight": 1.2, "test": lambda v: v < 0.26},
    "nose_avg_upper":   {"weight": 0.8, "test": lambda nose_y, avg_upper_y: nose_y > avg_upper_y},
    "angle":            {"weight": 1.2, "test": lambda angle: abs(angle) < 42},
    "hip_ankle":        {"weight": 1.0, "test": lambda hip_y, ankle_y: hip_y > ankle_y - 5},
    "y_std":            {"weight": 0.7, "test": lambda y_std: y_std < 48},
    "head_foot_dist":   {"weight": 0.8, "test": lambda d: d < 0.36},
    "center_y":         {"weight": 1.0, "test": lambda cy, h: cy > h * 0.67},
    "shoulder_hip_hip_ankle": {"weight": 1.0, "test": lambda s, ha: s < 0.21 and ha < 0.26},
    "hip_knee":         {"weight": 0.8, "test": lambda d: d < 0.24},
    "y_range":          {"weight": 1.0, "test": lambda r: r < 230},
}


# [ ì ìˆ˜ ì„ê³„ì¹˜ ] (íŠœë‹ í¬ì¸íŠ¸)
FALLEN_SCORE_THRESHOLD = 4.0


# [ í”„ë ˆì„ ì—°ì† ì„ê³„ì¹˜ ] (ê¸°ì¡´ ê°’ ì¬ì‚¬ìš©)
FALL_COUNT_THRESHOLD = 5          # ì—°ì† 5í”„ë ˆì„ ì´ìƒì´ë©´ í™•ì • ë‚™ìƒ
fall_status = {}                  # {track_id: {"count": int}}

def evaluate_criteria(kps_xy: np.ndarray, frame: np.ndarray):

    # kps_xy : (17, 2) í˜•íƒœì˜ x,y ì¢Œí‘œë§Œ (YOLO kpts.xy)
    # frame  : BGR frame (H,W,3)
    # return : (ê° ê·œì¹™ ê²°ê³¼ dict, ì´ì  float)

    xs, ys = kps_xy[:, 0], kps_xy[:, 1]
    width = float(np.max(xs) - np.min(xs) + 1e-5)
    height = float(np.max(ys) - np.min(ys) + 1e-5)
    aspect_ratio = height / width

    l_sh_y, r_sh_y = kps_xy[L_SHOULDER][1], kps_xy[R_SHOULDER][1]
    l_hip_y, r_hip_y = kps_xy[L_HIP][1], kps_xy[R_HIP][1]
    mean_sh_y = (l_sh_y + r_sh_y) / 2.0
    mean_hip_y = (l_hip_y + r_hip_y) / 2.0
    vertical_sh_hip = abs(mean_sh_y - mean_hip_y) / height

    nose_y = kps_xy[NOSE][1]
    avg_upper_y = np.mean([l_sh_y, r_sh_y, l_hip_y, r_hip_y])

    dx = ((kps_xy[L_SHOULDER][0] + kps_xy[R_SHOULDER][0]) / 2.0) - ((kps_xy[L_HIP][0] + kps_xy[R_HIP][0]) / 2.0)
    dy = mean_sh_y - mean_hip_y
    angle = np.degrees(np.arctan2(dy, dx))

    hip_y = min(l_hip_y, r_hip_y)
    ankle_y = min(kps_xy[L_ANKLE][1], kps_xy[R_ANKLE][1])

    y_std = float(np.std(ys))
    center_y = float(np.mean(ys))
    head_foot_dist = abs(nose_y - np.mean([kps_xy[L_ANKLE][1], kps_xy[R_ANKLE][1]])) / height

    shoulder_y = mean_sh_y
    hip_y2 = mean_hip_y
    ankle_y2 = float(np.mean([kps_xy[L_ANKLE][1], kps_xy[R_ANKLE][1]]))
    knee_y = float(np.mean([kps_xy[L_KNEE][1], kps_xy[R_KNEE][1]]))
    shoulder_hip_dist = abs(shoulder_y - hip_y2) / height
    hip_ankle_dist = abs(hip_y2 - ankle_y2) / height
    hip_knee_dist = abs(hip_y2 - knee_y) / height

    y_range = float(np.ptp(ys))
    h = frame.shape[0]

    feats = dict(
        aspect_ratio=aspect_ratio,
        vertical_sh_hip=vertical_sh_hip,
        nose_avg_upper=(nose_y, avg_upper_y),
        angle=angle,
        hip_ankle=(hip_y, ankle_y),
        y_std=y_std,
        head_foot_dist=head_foot_dist,
        center_y=(center_y, h),
        shoulder_hip_hip_ankle=(shoulder_hip_dist, hip_ankle_dist),
        hip_knee=hip_knee_dist,
        y_range=y_range
    )

    results = {}
    score = 0.0
    for k, cfg in CRITERIA.items():
        try:
            if k == "nose_avg_upper":
                v = cfg["test"](*feats["nose_avg_upper"])
            elif k == "hip_ankle":
                v = cfg["test"](*feats["hip_ankle"])
            elif k == "center_y":
                v = cfg["test"](*feats["center_y"])
            elif k == "shoulder_hip_hip_ankle":
                v = cfg["test"](*feats["shoulder_hip_hip_ankle"])
            else:
                v = cfg["test"](feats.get(k, 0))
        except Exception:
            v = False
        results[k] = bool(v)
        if v:
            score += float(cfg["weight"])

    return results, float(score)


# [ ì‚¬ê³  ê°ì§€ (ë‚™ìƒ) í•¨ìˆ˜ ]
def detect_acc(image_bytes):
    """ì‚¬ê³ (ë‚™ìƒ)ë¥¼ ê°ì§€í•˜ëŠ” í•¨ìˆ˜"""

    # YOLO Pose íŠ¸ë˜í‚¹ + ë‹¤ì¤‘ ê·œì¹™ ì ìˆ˜í™”ë¡œ ë‚™ìƒ ê°ì§€.
    # í”„ë ˆì„ë³„ ì ìˆ˜ >= FALLEN_SCORE_THRESHOLD ì´ë©´ í›„ë³´ë¡œ ì¹´ìš´íŠ¸ +1
    # ê°™ì€ track_idê°€ FALL_COUNT_THRESHOLD ì—°ì† ì¶©ì¡± ì‹œ 'ë‚™ìƒ í™•ì •'

    print("AI: ì‚¬ê³  ê°ì§€ ëª¨ë“ˆ í˜¸ì¶œ")
    
    image = Image.open(io.BytesIO(image_bytes))
    frame = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)

    detections = []
    violations_to_record = []

    # YOLO pose ì¶”ì  (ID ìœ ì§€)
    results = acc_model.track(frame, persist=True, conf=0.25, iou=0.5, max_det=50, verbose=False)

    for r in results:
        if r.boxes.id is None:
            continue

        boxes = r.boxes.xyxy.cpu().numpy().astype(int)
        track_ids = r.boxes.id.cpu().numpy().astype(int)

        # kpts.xy : (N, 17, 2) â€” ì¢Œí‘œë§Œ ì‚¬ìš©
        if r.keypoints is not None and hasattr(r.keypoints, "xy"):
            kpts_xy_all = r.keypoints.xy.cpu().numpy()
        else:
            kpts_xy_all = [None] * len(track_ids)

        for box, tid, kps_xy in zip(boxes, track_ids, kpts_xy_all):
            x1, y1, x2, y2 = [int(v) for v in box.tolist()]

            is_violation = False
            score = 0.0
            kps_xy_safe = []

            if kps_xy is not None and np.shape(kps_xy)[0] >= 17:
                # ê·œì¹™ í‰ê°€ (ì˜¤ë¥˜ ë‚˜ë©´ í›„ë³´ ë¬´íš¨ ì²˜ë¦¬)
                try:
                    conditions, score = evaluate_criteria(kps_xy, frame)
                    is_candidate = (score >= FALLEN_SCORE_THRESHOLD)
                except Exception:
                    is_candidate = False

                # ì—°ì† ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸
                now_ts = time.time()
                prev = fall_status.get(int(tid), {"count": 0})["count"]
                curr = prev + 1 if is_candidate else 0
                fall_status[int(tid)] = {"count": curr, "ts": now_ts}

                # ìµœì¢… í™•ì • ì¡°ê±´
                is_violation = (curr >= FALL_COUNT_THRESHOLD)
                kps_xy_safe = kps_xy
            else:
                # í‚¤í¬ì¸íŠ¸ê°€ ì—†ìœ¼ë©´ ë‚™ìƒ ì•„ë‹˜ìœ¼ë¡œ ì²˜ë¦¬
                fall_status[int(tid)] = {"count": 0}

            det = {
                "box": [x1, y1, x2, y2],
                "label": f"person (ID: {int(tid)})",
                "is_falling": bool(is_violation),
                "track_id": int(tid),
                "keypoints": _to_py(kps_xy_safe) if len(np.shape(kps_xy_safe)) > 0 else [],
                "source": "acc",    # íŠ¸ë˜í‚¹ ëª¨ë¸ : acc ëª¨ë¸
                "kind": "person"    # íŠ¸ë˜í‚¹ ëŒ€ìƒ : ì‚¬ëŒ
            }
            detections.append(det)

            if is_violation:
                violations_to_record.append({
                    "track_id": int(tid),
                    "labels": ["fallen"],   # í•„ìš” ì‹œ ["slip"], ["trip"] ë“±ìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥
                    "image": image,         # ì›ë³¸ PIL ì´ë¯¸ì§€
                    "detections": detections.copy()
                })

    return detections, violations_to_record
          


# -----------------------------------
#           ì¤‘ì¥ë¹„ ì¶œì… AI
# -----------------------------------

# [ ì¤‘ì¥ë¹„ ì¢…ë¥˜ë³„ ìƒ‰ìƒ ë§¤í•‘ ]
HE_COLOR_MAP = {
    'DumpTruck_1t_under': (255, 105, 180), # HotPink
    'DumpTruck_5t_under': (255, 0, 255),   # Magenta
    'DumpTruck_12t_under': (148, 0, 211),  # DarkViolet
    'DumpTruck_12t_over': (75, 0, 130),    # Indigo
    'Excavator_Tire': (0, 0, 255),         # Blue
    'Excavator_Crawler': (0, 128, 0),      # Green
    'Loader': (255, 255, 0),               # Yellow
    'Forklift': (255, 165, 0),             # Orange
    'ConcreteMixerTruck': (255, 0, 0),     # Red
    'Bulldozer': (128, 128, 128),          # Grey
    'DrillingMachine': (255, 20, 147),     # DeepPink
    'PileDriver': (0, 255, 255),           # Cyan
    'CargoTruck_1t_under': (128, 0, 0),    # Maroon
    'CargoTruck_5t_under': (0, 128, 128),  # Teal
    'CargoTruck_25t_under': (0, 0, 128),   # Navy
    'CargoTruck_25t_over': (128, 0, 128)   # Purple
}


# [ ì¤‘ì¥ë¹„ íƒì§€ í•¨ìˆ˜ ]
# íŠ¹ì • ì˜ì—­ì„ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ì¥ë¹„ì˜ ì¶œì…ì„ íƒì§€
he_event_recorded = {}  # track_id: "ì…ì°¨" or "ì¶œì°¨"

def detect_he(image_bytes):
    """ì¤‘ì¥ë¹„ íƒì§€ ë° OCRì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜"""
    print("AI: ì¤‘ì¥ë¹„ ê°ì§€ ëª¨ë“ˆ í˜¸ì¶œ")

    global heavy_equipment_ocr_attempts, last_ocr_call_ts, ocr_backoff_until, he_frame_counter
    he_frame_counter += 1

    image = Image.open(io.BytesIO(image_bytes))
    frame = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)

    # 1) HE ë¨¼ì € íƒì§€
    results = he_model.track(frame, persist=True, conf=0.25, iou=0.5, max_det=30, verbose=False)
    detections = []
    violations_to_record = []

    if not results or results[0].boxes.id is None or len(results[0].boxes.id) == 0:
        # HEê°€ ì—†ìœ¼ë©´ OCRë„ í˜¸ì¶œí•˜ì§€ ì•ŠìŒ
        return [], []

    # HE ë°•ìŠ¤/íŠ¸ë™ ëª¨ìœ¼ê¸°
    he_items = []
    names = results[0].names
    for box, track_id, cls in zip(results[0].boxes.xyxy.cpu(),
                                  results[0].boxes.id.cpu(),
                                  results[0].boxes.cls.cpu()):
        tid = int(track_id.item())
        class_name = names[int(cls.item())]
        he_box = list(map(int, box))
        he_items.append((he_box, tid, class_name))

    # 2) OCR í˜¸ì¶œ ì¡°ê±´ íŒë‹¨ (ì¿¨ë‹¤ìš´/ìŠ¤íŠ¸ë¼ì´ë“œ/ë°±ì˜¤í”„/ë²ˆí˜¸íŒ ë¯¸í™•ì • íŠ¸ë™ ì¡´ì¬ ì—¬ë¶€)
    need_ocr = any(heavy_equipment_ocr_attempts.get(tid, 0) < OCR_RETRY_THRESHOLD for _, tid, _ in he_items)
    ocr_results = []
    now = time.time()
    if need_ocr and now >= ocr_backoff_until \
       and (now - last_ocr_call_ts) >= OCR_COOLDOWN_SEC \
       and (he_frame_counter % FRAME_STRIDE_FOR_OCR == 0):
        

        h, w = frame.shape[:2]
        target_w = 1920
        scale = 1.0
        ocr_src = frame
        if w > target_w:
            scale = target_w / float(w)
            ocr_src = cv2.resize(frame, (int(w*scale), int(h*scale)), interpolation=cv2.INTER_AREA)

        ocr_results = call_hyperclova_ocr(ocr_src)
        last_ocr_call_ts = now

        # ì¢Œí‘œ ì›ë³µ
        if scale != 1.0:
            inv = 1.0 / scale
            for r in ocr_results:
                x1, y1, x2, y2 = r['box']
                r['box'] = [int(round(x1*inv)), int(round(y1*inv)),
                            int(round(x2*inv)), int(round(y2*inv))]


    # 3) HE ë°•ìŠ¤ - OCR í…ìŠ¤íŠ¸ ë§¤ì¹­ (í™•ì¥ ë°•ìŠ¤ + ì¤‘ì‹¬ì )
    H_EXPAND = 0.12           # ë°•ìŠ¤ í™•ì¥ ë¹„ìœ¨ 
    ABS_EXPAND_MIN = 12       # ìµœì†Œ 12pxì€ í™•ì¥
    h, w = frame.shape[:2]

    for he_box, track_id, class_name in he_items:
        he_last_seen[track_id] = now
        x1_he, y1_he, x2_he, y2_he = he_box
        bw, bh = (x2_he - x1_he), (y2_he - y1_he)

        # í™•ì¥ ë°•ìŠ¤ ê³„ì‚°
        mx = max(int(bw * H_EXPAND), ABS_EXPAND_MIN)
        my = max(int(bh * H_EXPAND), ABS_EXPAND_MIN)

        ex1 = max(0, x1_he - mx)
        ey1 = max(0, y1_he - my)
        ex2 = min(w - 1, x2_he + mx)
        ey2 = min(h - 1, y2_he + my)

        license_plate = None
        best_conf = -1.0

        # OCR ê²°ê³¼ê°€ ìˆì„ ë•Œë§Œ ë§¤ì¹­ ì‹œë„
        for o in ocr_results:
            x1o, y1o, x2o, y2o = o['box']
            cx = (x1o + x2o) / 2.0
            cy = (y1o + y2o) / 2.0

            if ex1 < cx < ex2 and ey1 < cy < ey2:
                conf = float(o.get('confidence', 0.5))
                # (ì„ íƒ) ë²ˆí˜¸íŒ í˜•íƒœ ê°„ë‹¨ í•„í„°ë¥¼ ë„£ê³  ì‹¶ìœ¼ë©´ ì—¬ê¸°ì—ì„œ regexë¡œ ê°€ì /ê°ì 
                if conf > best_conf:
                    best_conf = conf
                    license_plate = o['text']

        # 4) OCR ì¬ì‹œë„ ê´€ë¦¬
        if license_plate:
            print(f"  [HE] OCR ë§¤ì¹­ ì„±ê³µ! ë²ˆí˜¸: {license_plate}")
            heavy_equipment_ocr_attempts[track_id] = 0
        else:
            current_attempts = heavy_equipment_ocr_attempts.get(track_id, 0) + 1
            heavy_equipment_ocr_attempts[track_id] = current_attempts
            print(f"  [HE] OCR ë§¤ì¹­ ì‹¤íŒ¨. (ID: {track_id}, ì‹œë„: {current_attempts}/{OCR_RETRY_THRESHOLD})")
            
            if current_attempts >= OCR_RETRY_THRESHOLD:
                print(f"  [HE] OCR ì¬ì‹œë„ ì„ê³„ê°’ ë„ë‹¬. ID: {track_id}ë¥¼ 'ì¸ì‹ ì˜¤ë¥˜'ë¡œ ê¸°ë¡í•©ë‹ˆë‹¤.")
                license_plate = "ì¸ì‹ ì˜¤ë¥˜"
                heavy_equipment_ocr_attempts[track_id] = 0

        # 5) ê²°ê³¼ ì •ë¦¬
        if license_plate:
            access_status = get_access_status(track_id, y1_he, y2_he)

            last_recorded_status = he_event_recorded.get(track_id)

            # ìƒˆë¡œìš´ ì…/ì¶œì°¨ ì´ë²¤íŠ¸ì¸ì§€ í™•ì¸
            # ì´ì „ì— ê¸°ë¡ëœ ìƒíƒœì™€ í˜„ì¬ ìƒíƒœê°€ ë‹¤ë¥¼ ë•Œë§Œ ê¸°ë¡í•¨
            if last_recorded_status != access_status:
                print(f"âœ… ì¤‘ì¥ë¹„ '{access_status}' ì´ë²¤íŠ¸ ë°œìƒ! (ID: {track_id}, ë²ˆí˜¸: {license_plate}) ê¸°ë¡ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.")
                he_event_recorded[track_id] = access_status # í˜„ì¬ ìƒíƒœë¥¼ "ê¸°ë¡ëœ ìƒíƒœ"ë¡œ ì—…ë°ì´íŠ¸

                # violations_to_record ì— ì¶”ê°€í•˜ì—¬ Spring Bootë¡œ ì „ì†¡
                violations_to_record.append({
                    'track_id': track_id,
                    'info': {'heType': class_name, 'heNumber': license_plate, 'access': access_status},
                    'image': Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)),
                    'detections': he_items.copy() # í˜„ì¬ í”„ë ˆì„ì˜ HE íƒì§€ ê²°ê³¼ ì „ë‹¬
                })

            detections.append({
                'box': he_box,
                'label': f"{class_name} (ID: {track_id})",
                'heType': class_name,
                'heNumber': license_plate,
                'access': access_status,
                'track_id': track_id,
                'source': 'he',
                'kind': 'he'
            })
           
            print(f"âœ… ì¤‘ì¥ë¹„ íƒì§€ ì„±ê³µ! (ID: {track_id}, ë²ˆí˜¸: {license_plate}, ì…ì¶œì…: {access_status})")

    return detections, violations_to_record


# [ Track ID ë³‘í•© í•¨ìˆ˜ ]
def _iou(b1, b2):
    x1 = max(b1[0], b2[0]); y1 = max(b1[1], b2[1])
    x2 = min(b1[2], b2[2]); y2 = min(b1[3], b2[3])
    iw = max(0, x2 - x1); ih = max(0, y2 - y1)
    inter = iw * ih
    if inter <= 0:
        return 0.0
    a1 = (b1[2]-b1[0]) * (b1[3]-b1[1])
    a2 = (b2[2]-b2[0]) * (b2[3]-b2[1])
    return inter / max(1.0, float(a1 + a2 - inter))

def merge_and_dedupe(all_dets, iou_thr=0.55):
    acc_persons = []
    other_persons = []
    he_list = []
    gear_list = []

    for d in all_dets:
        kind = d.get("kind")
        if kind == "he" or ("heType" in d):
            he_list.append(d)
        elif kind == "gear":
            gear_list.append(d)  # ì¥ë¹„ ë°•ìŠ¤ ë³´ì¡´
        elif kind == "person":
            if d.get("source") == "acc":
                acc_persons.append(d)
            else:
                other_persons.append(d)

    merged = []

    # 1) ACC ì‚¬ëŒ ìš°ì„ 
    for p in acc_persons:
        merged.append(p.copy())

    # 2) PPE ì‚¬ëŒì„ ACCì™€ IoUë¡œ ë³‘í•©
    for q in other_persons:
        qbox = q.get("box")
        qtid = q.get("track_id")
        if not qbox:
            continue

        best_iou, best_idx = 0.0, -1
        for i, m in enumerate(merged):
            if m.get("kind") != "person":
                continue
            mbox = m.get("box")
            if not mbox:
                continue
            # ê°™ì€ track_id ì¸ ê²½ìš°, ë°”ë¡œ ë§¤ì¹­
            if qtid is not None and m.get("track_id") == qtid:
                best_iou, best_idx = 1.0, i
                break
            iou = _iou(qbox, mbox)
            if iou > best_iou:
                best_iou, best_idx = iou, i

        if best_iou >= iou_thr and best_idx >= 0:
            m = merged[best_idx]
            ss = set(m.get("safety_status", [])) | set(q.get("safety_status", []))
            if ss:
                m["safety_status"] = list(ss)
            m["is_falling"] = bool(m.get("is_falling")) or bool(q.get("is_falling"))
            if (m.get("source") != "acc") and q.get("keypoints"):
                m["keypoints"] = q["keypoints"]
            merged[best_idx] = m
        else:
            merged.append(q.copy())

    # 3) HE, GEAR ê·¸ëŒ€ë¡œ ë§ë¶™ì´ê¸° 
    # gear ë¥¼ ë§ˆì§€ë§‰ì— ë‘ë©´ ì‚¬ëŒ ë°•ìŠ¤ ìœ„ì— ì˜ ë³´ì„
    merged.extend(he_list)
    merged.extend(gear_list)

    return merged


# [ HyperCLOVA OCR API í˜¸ì¶œ í•¨ìˆ˜ ]
# ë°”ìš´ë”© ë°•ìŠ¤ ì˜ë¼ì„œ í•´ë‹¹ ì˜ì—­ì— HyperCLOVA OCR API ì„ í˜¸ì¶œí•˜ì—¬ ë²ˆí˜¸íŒ íƒì§€
def call_hyperclova_ocr(image):
    """HyperCLOVA OCR APIë¥¼ í˜¸ì¶œí•˜ì—¬ ë²ˆí˜¸íŒì„ íƒì§€í•˜ëŠ” í•¨ìˆ˜"""
    global ocr_backoff_until

    # 1. NCP HyperClova OCR API í‚¤ ë° URL ì„¤ì •
    if not OCR_API_URL or not OCR_SECRET_KEY:
        print("âŒ OCR ì„¤ì • ëˆ„ë½(HYPERCLOVA_OCR_URL / HYPERCLOVA_OCR_SECRET)")
        ocr_backoff_until = time.time() + 10
        return []    
    

    # 2. ì´ë¯¸ì§€ë¥¼ Base64ë¡œ ì¸ì½”ë”©
    image_pil = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    img_byte_arr = io.BytesIO()
    image_pil.save(img_byte_arr, format='JPEG')
    img_base64 = base64.b64encode(img_byte_arr.getvalue()).decode('utf-8')

    # 3. API ìš”ì²­ ë°”ë”” ìƒì„±
    request_body = {
        "version": "V1",
        "requestId": "unique-request-id",
        "timestamp": int(round(time.time() * 1000)),
        "images": [
            {
                "format": "jpeg",
                "name": "image_name",
                "data": img_base64
            }
        ]
    }
    
    headers = {
        "X-OCR-SECRET": OCR_SECRET_KEY,
        "Content-Type": "application/json"
    }

    try:
        response = requests.post(
            OCR_API_URL, 
            headers=headers, 
            json=request_body, 
            timeout=(1.0, 2.5), 
            allow_redirects=False
        )
        response.raise_for_status()     # HTTP ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ì˜ˆì™¸ ë°œìƒ
        result = response.json()
        
        # 4. API ì‘ë‹µì—ì„œ ë²ˆí˜¸íŒ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        ocr_results = []
        for image_result in result.get('images', []):
            for field in image_result.get('fields', []):
                
                # í¬ë˜ì‹œ / ëŠê¹€ ë°©ì§€ ì•ˆì „ì¥ì¹˜
                poly = field.get('boundingPoly', {})
                vertices = poly.get('vertices') or []
                if len(vertices) < 4:
                    continue

                # API ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ì™€ ê¼­ì§€ì (vertex) ì¢Œí‘œë¥¼ ì¶”ì¶œ
                vertices = field['boundingPoly']['vertices']
                
                # ê¼­ì§€ì  ì¢Œí‘œë“¤ë¡œë¶€í„° ì‚¬ê° ë°”ìš´ë”© ë°•ìŠ¤(x1, y1, x2, y2)ë¥¼ ê³„ì‚°
                x_coords = [v['x'] for v in vertices]
                y_coords = [v['y'] for v in vertices]
                box = [min(x_coords), min(y_coords), max(x_coords), max(y_coords)]
                conf = field.get('inferConfidence', 0.5)
                try:
                    conf = float(conf)
                except Exception:
                    conf = 0.5
                
                # í…ìŠ¤íŠ¸ì™€ ê³„ì‚°ëœ ë°•ìŠ¤ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë¬¶ì–´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                ocr_results.append({
                    'text': field['inferText'],
                    'box': box,
                    'confidence': conf
                })
        
        return ocr_results # ìµœì¢…ì ìœ¼ë¡œ [{'text': 'í…ìŠ¤íŠ¸', 'box': [x1,y1,x2,y2]}, ...] í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜
            
    except requests.exceptions.RequestException as e:
        print(f"âŒ OCR API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        # âœ… ì‹¤íŒ¨ ì‹œ 10ì´ˆ ë°±ì˜¤í”„
        ocr_backoff_until = time.time() + 10
        return [] # ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜
    except Exception as e:
        print(f"âŒ OCR ì‘ë‹µ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        ocr_backoff_until = time.time() + 10
        return [] # ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜
    finally:
        eventlet.sleep(0)   # ê¸´ I/O ë’¤ì—” ë°˜ë“œì‹œ ì–‘ë³´í•´ì„œ ì†Œì¼“ í•˜íŠ¸ë¹„íŠ¸ ì‚´ë¦¬ê¸°


# [ ì¤‘ì¥ë¹„ ì…ì¶œì… íŒë³„ í•¨ìˆ˜ ]
# yì¶• ìœ„ì¹˜ì˜ ë³€í™”ë¥¼ ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨í•˜ì—¬, ìœ„ë¡œ ì˜¬ë¼ê°€ë©´ 'ì…ì°¨'(in), ì•„ë˜ë¡œ ë‚´ë ¤ê°€ë©´ 'ì¶œì°¨'(out)
heavy_equipment_history = {}
Y_DELTA_MIN = 3             # ìµœì†Œ í”½ì…€ ì„ê³„
Y_DELTA_MAX = 24            # ê³¼ë„í•œ ì í”„ ì œí•œ 
Y_DELTA_PCT = 0.018          # ë°•ìŠ¤ ë†’ì´ì˜ 1.8% 

def get_access_status(track_id, current_y1, current_y2):
    """íŠ¸ë˜í‚¹ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì…ì¶œì… ìƒíƒœë¥¼ íŒë³„í•˜ëŠ” í•¨ìˆ˜"""
    global heavy_equipment_history
    current_y_center = (current_y1 + current_y2) / 2.0
    box_h = max(1, current_y2 - current_y1)
    local_thr = max(Y_DELTA_MIN, min(Y_DELTA_MAX, int(box_h * Y_DELTA_PCT)))

    hist = heavy_equipment_history.get(track_id)
    now_ts = time.time()

    if not hist:
        heavy_equipment_history[track_id] = {'last_y_center': current_y_center, 'last_status': 'ì…ì°¨', 'ts': now_ts}
        return 'ì…ì°¨' # ì´ˆê¸° ì§„ì…ì€ 'ì…ì°¨'

    dy = current_y_center - hist['last_y_center']
    # ì„ê³„ ì´í•˜ ì›€ì§ì„ì€ ì´ì „ ìƒíƒœ ìœ ì§€, ë„˜ìœ¼ë©´ ë°©í–¥ìœ¼ë¡œ íŒì •
    status = hist['last_status'] if abs(dy) < local_thr else ('ì…ì°¨' if dy < 0 else 'ì¶œì°¨')


    hist['last_y_center'] = current_y_center
    hist['last_status'] = status
    hist['ts'] = now_ts 
    return status


# =================================================================
#           3. Spring Boot ë° NCP Object Storage ì—°ë™ ì„¤ì •
# =================================================================
SPRING_BOOT_API_URL = os.getenv("SPRING_BOOT_API_URL", "http://10.1.20.7:8090/web/aiapi")
NCP_ENDPOINT_URL = os.getenv("NCP_ENDPOINT_URL", "https://kr.object.ncloudstorage.com")
NCP_BUCKET_NAME = os.getenv("NCP_BUCKET_NAME", "aivis-obj-storage")
NCP_ACCESS_KEY = os.getenv("NCP_ACCESS_KEY")      
NCP_SECRET_KEY = os.getenv("NCP_SECRET_KEY")      
NCP_REGION = os.getenv("NCP_REGION", "kr-standard")


# í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ í™•ì¸ 
missing = [k for k, v in {
    "NCP_ACCESS_KEY": NCP_ACCESS_KEY,
    "NCP_SECRET_KEY": NCP_SECRET_KEY,
}.items() if not v]
if missing:
    raise RuntimeError(f"Missing required env vars: {', '.join(missing)}")
    

# NCP Object Storage í´ë¼ì´ì–¸íŠ¸ (Boto3)
s3_client = boto3.client(
    's3',
    endpoint_url=NCP_ENDPOINT_URL,
    aws_access_key_id=NCP_ACCESS_KEY,
    aws_secret_access_key=NCP_SECRET_KEY,
    region_name=NCP_REGION
)


# HyperCLOVA OCR 
OCR_API_URL    = os.getenv("HYPERCLOVA_OCR_URL")    
OCR_SECRET_KEY = os.getenv("HYPERCLOVA_OCR_SECRET")


# ì¶”ì ëœ ê°ì²´ë³„ë¡œ ë§ˆì§€ë§‰ ê¸°ë¡ ì‹œê°„ì„ ì €ì¥í•˜ì—¬ ì¤‘ë³µ ê¸°ë¡ ë°©ì§€
last_recorded_times = {}
RECORD_COOLDOWN = 300            # ì´ˆ ë‹¨ìœ„, ë™ì¼ ê°ì²´ì— ëŒ€í•´ 20ì´ˆì— í•œ ë²ˆë§Œ ê¸°ë¡ ì „ì†¡


# =================================================================
# 4. WebSocket ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
# =================================================================

client_main_device: dict[str, Union[int, None]] = {}

# [ í´ë¼ì´ì–¸íŠ¸(React) ì—°ê²° ì„±ê³µ ì‹œ, ì‹¤í–‰ í•¨ìˆ˜ ]
@socketio.on('connect')
def on_connect():
    client_main_device[request.sid] = None
    print(f"âœ… Client connected: {request.sid}")


# [ í´ë¼ì´ì–¸íŠ¸(React) ì—°ê²° ì¢…ë£Œ ì‹œ, ì‹¤í–‰ í•¨ìˆ˜]
@socketio.on('disconnect')
def on_disconnect():
    sid = request.sid
    client_main_device.pop(sid, None)

    stop_video_stream(owner_sid=sid)

    print(f"âŒ Client disconnected: {sid}")


# [ ë©”ì¸ í™”ë©´ ì¥ë¹„ ì„¤ì • ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ]
@socketio.on('set_main_device')
def handle_set_main_device(data):
    raw = data.get('deviceId', None)
    try:
        device_id = int(raw)
    except (TypeError, ValueError):
        emit('server_error', {'type': 'missing_deviceId', 'sid': request.sid}, to=request.sid)
        return

    client_main_device[request.sid] = device_id
    print(f"ğŸ“º Main device set to: {device_id} (sid={request.sid})")

    emit('main_device_set', {'ok': True, 'deviceId': device_id}, to=request.sid)


# [ ì „ì—­ íƒì§€ ê¸°ëŠ¥ On/Off í† ê¸€ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ]
@socketio.on('toggle_global_detection')
def handle_toggle_global_detection(data):
    """React í•˜ë‹¨ ë²„íŠ¼ìœ¼ë¡œ ì „ì—­ íƒì§€ ê¸°ëŠ¥ On/Off ì‹œ í˜¸ì¶œë  í•¨ìˆ˜"""
    global global_toggle_states
    detection_type = data.get('detectionType')              # 'ppe', 'acc', 'he'
    is_active = data.get('isActive')

    if detection_type in global_toggle_states and isinstance(is_active, bool):
        global_toggle_states[detection_type] = is_active
        print(f"ğŸŒ Global toggles updated: {global_toggle_states}")
        # í´ë¼ì´ì–¸íŠ¸ì— ìƒíƒœê°€ ì—…ë°ì´íŠ¸ ì•Œë¦¼ (ì„ íƒ ì‚¬í•­)
        emit('global_state_updated', global_toggle_states, broadcast=True)
        eventlet.sleep(0)


# [ ë™ì˜ìƒ ì²˜ë¦¬ ìŠ¤ë ˆë“œë¥¼ ì•ˆì „í•˜ê²Œ ì¢…ë£Œí•˜ëŠ” í•¨ìˆ˜ ]
def stop_video_stream(owner_sid=None):
    """ë™ì˜ìƒ ì²˜ë¦¬ ìŠ¤ë ˆë“œë¥¼ ì•ˆì „í•˜ê²Œ ì¢…ë£Œí•˜ëŠ” í•¨ìˆ˜"""
    global current_video_stream, thread_for_video_processing, current_stream_owner_sid

    # ì†Œìœ ì í™•ì¸í•˜ì—¬ owner_sid ì™€ í˜„ì¬ ì†Œìœ ìì™€ ë‹¤ë¥´ë©´ ì¢…ë£Œí•˜ì§€ ì•ŠìŒ
    if owner_sid is not None and current_stream_owner_sid != owner_sid:
        return
    
    if current_video_stream:
        print("ë™ì˜ìƒ ìŠ¤íŠ¸ë¦¼ ì¤‘ì§€ ìš”ì²­")
        try:
            current_video_stream.release()
        except Exception:
            pass
        current_video_stream = None          # ìŠ¤ë ˆë“œ ì¢…ë£Œë¥¼ ìœ„í•´ None ìœ¼ë¡œ ì„¤ì •

    # None í• ë‹¹ìœ¼ë¡œ ë£¨í”„ ì¢…ë£Œ ìœ ë„
    thread_for_video_processing = None
    current_stream_owner_sid = None


# [ Reactë¡œë¶€í„° ì˜ìƒ ë¶„ì„ ìš”ì²­ ì‹œ, ì‹¤í–‰ í•¨ìˆ˜]
@socketio.on('start_analysis')
def handle_start_analysis(data):
    """ì˜ìƒ ë¶„ì„ ì‹œì‘ ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ (ì›¹ìº  / ë™ì˜ìƒ ëª¨ë“œ)"""
    mode = data.get('mode')                     # 'webcam' ë˜ëŠ” 'íŒŒì¼ê²½ë¡œ'
    device_id = data.get('deviceId') 
    sid = request.sid

    if device_id is not None:
        try:
            client_main_device[sid] = int(device_id)
        except Exception:
            pass


    # ìƒˆë¡œìš´ ë¶„ì„ ì‹œì‘ ì „, ê¸°ì¡´ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì¤‘ì§€
    stop_video_stream(owner_sid=sid)

    if mode and mode != 'webcam':               # ë™ì˜ìƒ ëª¨ë“œ (íŒŒì¼ ê²½ë¡œ ì „ë‹¬)
        print(f"ë™ì˜ìƒ ëª¨ë“œ ì‹œì‘ ìš”ì²­: {mode} (Device: {device_id}, sid={sid})")
        start_video_processing(mode, device_id, sid)  

    elif mode == 'webcam':                      # ì›¹ìº  ëª¨ë“œ
        print("ì›¹ìº  ëª¨ë“œ ì¤€ë¹„ ì™„ë£Œ.")
        emit('analysis_started', {'status': 'webcam_ready'}, to=sid)


# [ Reactë¡œë¶€í„° ì´ë¯¸ì§€ ë¶„ì„ ìš”ì²­ ì‹œ, ì‹¤í–‰ í•¨ìˆ˜ ]
@socketio.on('image_analysis_request')
def handle_image_analysis(data):
    """Reactì—ì„œ ì›¹ìº  ì´ë¯¸ì§€ í”„ë ˆì„ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
    if not isinstance(data, dict):
        print("âš ï¸ invalid payload:", type(data))
        return

    image_data_url = data.get('image')
    device_id = data.get('deviceId')

    sid = request.sid
    try:
        device_id_int = int(device_id)
    except (TypeError, ValueError):
        print("âš ï¸ invalid deviceId:", device_id)
        return

    # í•´ë‹¹ ì„¸ì…˜ì´ í˜„ì¬ ì„ íƒí•œ ë©”ì¸ ë””ë°”ì´ìŠ¤
    selected = client_main_device.get(sid)
    print(f"[image_analysis_request] sid={sid}, device_id={device_id_int}, selected={selected}, toggles={global_toggle_states}")

    # ê¸°ëŠ¥ ì „ì›ì´ ì „ë¶€ OFFë©´ ìŠ¤í‚µ
    if not any(global_toggle_states.values()):
        print("â†©ï¸  skip: all toggles off")
        return

    # ì„¸ì…˜ì´ ì„ íƒí•œ ë©”ì¸ ë””ë°”ì´ìŠ¤ê°€ ì•„ë‹ˆë©´ ìŠ¤í‚µ
    try:
        if selected is None or device_id_int != int(selected):
            print("â†©ï¸  skip: not this sid's main device")
            return
    except Exception:
        print("â†©ï¸  skip: invalid selected value:", selected)
        return

    # ì´ë¯¸ì§€ ë°ì´í„° ê²€ì¦
    if not image_data_url or "," not in image_data_url:
        print("â†©ï¸  skip: bad image data")
        return

    # ì„œë²„ ì¸¡ ìŠ¤ë¡œí‹€ (ì¤‘ë³µ/ê³¼ì† ì „ì†¡ ë°©ì§€) 
    key = (sid, device_id_int)
    now = time.time()

    # 1) ë„ˆë¬´ ë¹ ë¥¸ ì¤‘ë³µ í”„ë ˆì„ ë“œë¡­
    if now - last_frame_ts.get(key, 0) < MIN_INTERVAL:
        return
    last_frame_ts[key] = now

    # 2) ì´ì „ í”„ë ˆì„ ì²˜ë¦¬ ì¤‘ì´ë©´ ë“œë¡­ (ë™ì‹œ ì²˜ë¦¬ ë°©ì§€)
    if key in processing_inflight:
        return
    processing_inflight.add(key)


    try:
        header, encoded = image_data_url.split(",", 1)
        image_bytes = base64.b64decode(encoded)
        original_image = Image.open(io.BytesIO(image_bytes))

        all_detections = []

        start_ts = time.time()

        jobs = []
        types = []
        
        if global_toggle_states.get('ppe'):
            jobs.append(GREEN_POOL.spawn(detect_ppe, image_bytes))
            types.append('ppe')
        if global_toggle_states.get('acc'):
            jobs.append(GREEN_POOL.spawn(detect_acc, image_bytes))
            types.append('acc')
        if global_toggle_states.get('he'):
            jobs.append(GREEN_POOL.spawn(detect_he, image_bytes))
            types.append('he')

        all_detections = []
        # ë³‘ë ¬ë¡œ ë‚ ë ¤ë‘” ì‘ì—…ì„ ìˆœì„œëŒ€ë¡œ íšŒìˆ˜(wait)
        for job, typ in zip(jobs, types):
            d, v = job.wait()  # ì—¬ê¸°ì„œ ê° ëª¨ë¸ ê²°ê³¼ ìˆ˜ì§‘
            all_detections.extend(d)
            socketio.start_background_task(process_violations, typ, v, original_image.copy(), device_id) 


        merged = merge_and_dedupe(all_detections)
        ai_detections = _to_py(merged)


        # ì„œë²„ê°€ ê¶Œì¥í•˜ëŠ” ë‹¤ìŒ ì „ì†¡ ê°„ê²© íŒíŠ¸ (ë°€ë¦¬ì´ˆ)
        proc_ms = int((time.time() - start_ts) * 1000)
        next_delay_ms = max(
            int(1000.0 / SERVER_TARGET_FPS),          # ëª©í‘œ FPS
            int(MIN_INTERVAL * 1000),                 # ì„œë²„ ì“°ë¡œí‹€
            int(proc_ms * 0.6)                        # ì²˜ë¦¬ì‹œê°„ ê¸°ë°˜ (ì•½ê°„ ì—¬ìœ )
        )

        emit('analysis_result', 
             {'detections': ai_detections, 
              'deviceId': int(device_id),
              'nextDelayMs': next_delay_ms
              }, to=sid)


    except Exception as e:
        print(f"ì´ë¯¸ì§€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        processing_inflight.discard(key)
        try:
            eventlet.sleep(0)
        except Exception:
            pass


# [ ë™ì˜ìƒì˜ AI ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” ìŠ¤ë ˆë“œ ì‹œì‘ í•¨ìˆ˜]
def start_video_processing(video_path, device_id, sid):
    """ë™ì˜ìƒì„ ì½ê³  AI ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” ìŠ¤ë ˆë“œë¥¼ ì‹œì‘í•˜ëŠ” í•¨ìˆ˜"""
    global thread_for_video_processing, current_stream_owner_sid

    # ë‹¤ë¥¸ ì„¸ì…˜ì´ ì†Œìœ  ì¤‘ì´ë©´ ê·¸ ì„¸ì…˜ ìŠ¤íŠ¸ë¦¼ë§Œ ì¢…ë£Œ
    if current_stream_owner_sid and current_stream_owner_sid != sid:
        stop_video_stream(owner_sid=current_stream_owner_sid)

    current_stream_owner_sid = sid
    thread_for_video_processing = eventlet.spawn(process_video_stream, video_path, device_id, sid)


# [ ë™ì˜ìƒì˜ AI ë¶„ì„ì„ ìˆ˜í–‰í•˜ì—¬ Reactë¡œ í”„ë ˆì„ì„ ë³´ë‚´ëŠ” í•¨ìˆ˜ ]
def process_video_stream(video_path, device_id, sid):
    """ë™ì˜ìƒì„ ì½ê³  AI ë¶„ì„ì„ ìˆ˜í–‰í•˜ì—¬ Reactë¡œ í”„ë ˆì„ì„ ë³´ë‚´ëŠ” í•¨ìˆ˜"""
    global current_video_stream, current_stream_owner_sid
    cap = cv2.VideoCapture(video_path)
    current_video_stream = cap

    try:
        while current_video_stream is cap:
            
            # ì†Œìœ ì í™•ì¸ í›„, ì†Œìœ ìê°€ ì•„ë‹ˆë©´ ì¢…ë£Œ
            if current_stream_owner_sid != sid:
                break

            # ì„¸ì…˜ë³„ í˜„ì¬ ì„ íƒëœ ë©”ì¸ ë””ë°”ì´ìŠ¤ í™•ì¸
            selected = client_main_device.get(sid)

            # AI ê¸°ëŠ¥ ìƒíƒœê°€ OFFì´ê±°ë‚˜, ì´ ì„¸ì…˜ì˜ ë©”ì¸ ë””ë°”ì´ìŠ¤ê°€ ì•„ë‹ˆë©´ ë¶„ì„ ìŠ¤í‚µ (í”„ë ˆì„ì€ ì†Œëª¨)
            if (not any(global_toggle_states.values())) or (selected is None) or (int(device_id) != int(selected)):
                ret, _ = cap.read()
                if not ret:
                    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                eventlet.sleep(0.05)    # í•˜íŠ¸ ë¹„íŠ¸ ìœ ì§€
                continue

            # í”„ë ˆì„ ì½ê¸°
            ret, frame = cap.read()
            if not ret:
                print(f"ë™ì˜ìƒ ìŠ¤íŠ¸ë¦¼ ë ({device_id}), ì¬ì‹œì‘í•©ë‹ˆë‹¤.")
                cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                eventlet.sleep(0)
                continue

            try:
                # OpenCV í”„ë ˆì„(numpy.ndarray)ì„ PIL Imageë¡œ ë³€í™˜
                pil_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))

                # PIL Imageë¥¼ ë©”ëª¨ë¦¬ ë²„í¼ë¡œ ë³€í™˜í•˜ì—¬ AI í•¨ìˆ˜ì— ì „ë‹¬
                buf = io.BytesIO()
                pil_img.save(buf, format='JPEG')
                image_bytes = buf.getvalue()

                all_detections = []

                # [ í™œì„±í™”ëœ ê¸°ëŠ¥ì— ëŒ€í•´ì„œë§Œ AI ë¶„ì„ ìˆ˜í–‰ ]
                start_ts = time.time()

                jobs = []
                types = []
                if global_toggle_states.get('ppe'):
                    jobs.append(GREEN_POOL.spawn(detect_ppe, image_bytes))
                    types.append('ppe')
                if global_toggle_states.get('acc'):
                    jobs.append(GREEN_POOL.spawn(detect_acc, image_bytes))
                    types.append('acc')
                if global_toggle_states.get('he'):
                    jobs.append(GREEN_POOL.spawn(detect_he, image_bytes))
                    types.append('he')

                all_detections = []
                for job, typ in zip(jobs, types):
                    d, v = job.wait()
                    all_detections.extend(d)
                    socketio.start_background_task(process_violations, typ, v, pil_img.copy(), device_id)


                merged = merge_and_dedupe(all_detections)
                ai_detections = _to_py(merged)

                proc_ms = int((time.time() - start_ts) * 1000)
                next_delay_ms = max(
                    int(1000.0 / SERVER_TARGET_FPS),
                    int(MIN_INTERVAL * 1000),
                    int(proc_ms * 0.6)
                )

                socketio.emit('analysis_result', 
                              {'detections': ai_detections, 
                               'deviceId': int(device_id),
                               'nextDelayMs': next_delay_ms
                               }, to=sid)

            except Exception as e:
                print(f"ë™ì˜ìƒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            finally:
                eventlet.sleep(0)  # í•‘/í ìœ ì§€

    finally:
        cap.release()
        if current_video_stream is cap:
            current_video_stream = None
        if current_stream_owner_sid == sid:
            current_stream_owner_sid = None
        print("ë™ì˜ìƒ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ì¢…ë£Œ")



# =================================================================
# 5. Spring Boot API ì—°ë™ ë° Object Storage ì—…ë¡œë“œ í•¨ìˆ˜
# =================================================================

# [ ìœ„ë°˜ ê¸°ë¡ ì²˜ë¦¬ í•¨ìˆ˜ ]
def process_violations(detection_type, violations_to_record, original_image, device_id):
    """íƒì§€ëœ ìœ„ë°˜ ì‚¬í•­ì„ ì¿¨ë‹¤ìš´ ì ìš©í•˜ì—¬ ê¸°ë¡í•˜ëŠ” í•¨ìˆ˜"""
    global last_recorded_times
    device_id = int(device_id)
    current_time = time.time()
    for violation in violations_to_record:
        # ã…‡ track_idê°€ ì—†ëŠ” ê²½ìš° (e.g. OCR ì‹¤íŒ¨)ë¥¼ ëŒ€ë¹„í•´ ê¸°ë³¸ê°’ 0 ì‚¬ìš©
        track_id = violation.get('track_id', 0)
        
        # ã…‡ ì¤‘ë³µ ê¸°ë¡ ë°©ì§€ë¥¼ ìœ„í•´ ê³ ìœ  í‚¤ ìƒì„± (íƒì§€íƒ€ì… + íŠ¸ë™ID)
        record_key = f"{detection_type}-device-{device_id}-track-{track_id}"
        last_time = last_recorded_times.get(record_key, 0)

        if current_time - last_time > RECORD_COOLDOWN:
            last_recorded_times[record_key] = current_time
            info = {}
            if detection_type in ('ppe', 'acc'):
                info = {'labels': violation['labels']}
            elif detection_type == 'he':
                info = violation['info']
            
            # record_violation í˜¸ì¶œ ì‹œì—ëŠ” ì „ì²´ íƒì§€ ê²°ê³¼ë¥¼ ë„˜ê²¨ì¤Œ
            record_violation(detection_type, info, original_image, track_id, violation['detections'], device_id)


# [ NCP ì˜¤ë¸Œì íŠ¸ ìŠ¤í† ë¦¬ì§€ì— ì—…ë¡œë“œ ]
def upload_to_ncp_storage(image_pil, filename, detection_type):
    try:
        in_mem_file = io.BytesIO()
        image_pil.save(in_mem_file, format='JPEG')
        in_mem_file.seek(0)


        # í•œêµ­ ì‹œê°„(KST) ê¸°ì¤€ ë‚ ì§œ
        now_kst = datetime.now(ZoneInfo("Asia/Seoul"))
        date_path = now_kst.strftime("%Y%m%d")

        # Key êµ¬ì„±: records/íƒì§€ìœ í˜•/20250808/íŒŒì¼ëª….jpg
        key = f"records/{detection_type}/{date_path}/{filename}"

        s3_client.upload_fileobj(
            in_mem_file,
            NCP_BUCKET_NAME,
            key,
            ExtraArgs={'ACL': 'public-read'}
        )
        print(f"  ğŸ“¤ íŒŒì¼ ì—…ë¡œë“œ ì„±ê³µ: {key}")
        return f"{NCP_ENDPOINT_URL}/{NCP_BUCKET_NAME}/{key}"
    except Exception as e:
        print(f"  âŒ íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None
    
    finally:
        # ì—…ë¡œë“œ í›„ ì–‘ë³´
        try:
            eventlet.sleep(0)
        except Exception:
            pass


def record_violation(detection_type, violation_info, original_image, track_id, detections, device_id):
    """AI íƒì§€ ìœ„ë°˜ ì‚¬í•­ì„ Spring Boot ì„œë²„ë¡œ ì „ì†¡í•˜ëŠ” í•¨ìˆ˜"""
    print(f"ğŸš¨ ìœ„ë°˜ ê°ì§€ (ID: {track_id}): {detection_type}. Spring Boot ì„œë²„ë¡œ ê¸°ë¡ì„ ì „ì†¡í•©ë‹ˆë‹¤")
    
    # í•œêµ­ ì‹œê°„(KST) ì§ì ‘ ê°€ì ¸ì˜¤ê¸°
    now_kst = datetime.now(ZoneInfo("Asia/Seoul"))


    # ã…‡ ì´ë¯¸ì§€ URL ë° íŒŒì¼ëª… ìƒì„±
    type_code_map = {"ppe": "01", "acc": "02", "he": "03"}
    type_code = type_code_map.get(detection_type, "00")
    timestamp = now_kst.strftime("%Y%m%d_%H%M%S")
    capture_filename = f"{type_code}_capture_{timestamp}.jpg"
    detection_filename = f"{type_code}_detect_{timestamp}.jpg"

    # ã…‡ íƒì§€ ê²°ê³¼ ì´ë¯¸ì§€ ìƒì„±
    detection_image = original_image.copy()
    draw = ImageDraw.Draw(detection_image)

    # ã…‡ í°íŠ¸ ì¢…ë¥˜ì™€ í¬ê¸° ì„¤ì •
    font = load_korean_font(size=24)

    HIGHLIGHT_COLOR = (255, 165, 0)   # orange, ê¸°ë¡ ëŒ€ìƒ ê°ì²´ ìƒ‰ìƒ
    VIOLATOR_COLOR = (255, 0, 0)      # red, ìœ„ë°˜ì ê°ì²´ ìƒ‰ìƒ
    NORMAL_COLOR = (0, 0, 255)        # blue, ì •ìƒ ê°ì²´ ìƒ‰ìƒ


    # ë°”ìš´ë”© ë°•ìŠ¤ ìƒ‰ìƒ ì§€ì • ë¡œì§
    for det in detections:
        box = det['box']
        label = det['label']

        # 1. ê¸°ë³¸ ìƒ‰ìƒ ê²°ì •
        color = NORMAL_COLOR # ê¸°ë³¸ê°’ì€ íŒŒë€ìƒ‰(ì •ìƒ)
        is_violator = False
        if 'safety_status' in det and any(status in SAFETY_LABELS['unwear'] for status in det['safety_status']):
            is_violator = True
        elif 'is_falling' in det and det.get('is_falling'):
            is_violator = True
        
        if is_violator:
            color = VIOLATOR_COLOR # ìœ„ë°˜ìë©´ ë¹¨ê°„ìƒ‰


        # 2. ì¤‘ì¥ë¹„ëŠ” ê³ ìœ  ìƒ‰ìƒ ê·œì¹™ ì ìš©
        if 'heType' in det:
            he_type = det.get('heType')
            color = HE_COLOR_MAP.get(he_type, VIOLATOR_COLOR)
        

        # 3. í˜„ì¬ ê¸°ë¡ ëŒ€ìƒì´ë©´ ì£¼í™©ìƒ‰ìœ¼ë¡œ ê°•ì¡°
        if det.get('track_id') == track_id:
            color = HIGHLIGHT_COLOR


        # 4. ìµœì¢… ê²°ì •ëœ ìƒ‰ìƒìœ¼ë¡œ ê·¸ë¦¬ê¸° ì‹¤í–‰
        draw.rectangle(box, outline=color, width=4)                     # ì„  êµµê¸° 4
        draw.text((box[0], box[1] - 28), label, fill=color, font=font)  # y ìœ„ì¹˜ì™€ í°íŠ¸ ì ìš©
        

        # 5. í¬ì¦ˆ í‚¤í¬ì¸íŠ¸ ì‹œê°í™” (ì‚¬ê³  ê°ì§€ ì‹œ)
        if detection_type == 'acc':
            kpts = det.get('keypoints', [])
            for kpt in kpts:
                draw.ellipse((kpt[0]-3, kpt[1]-3, kpt[0]+3, kpt[1]+3), fill=color, outline=color)


    # Object Storageì— ì´ë¯¸ì§€ ì—…ë¡œë“œ
    original_img_url = upload_to_ncp_storage(original_image, capture_filename, detection_type)
    detect_img_url = upload_to_ncp_storage(detection_image, detection_filename, detection_type)
    if not original_img_url or not detect_img_url:
        print("  âŒ ì´ë¯¸ì§€ URL ìƒì„± ì‹¤íŒ¨. ê¸°ë¡ ì „ì†¡ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return


    # Spring Boot APIë¡œ ë³´ë‚¼ ë°ì´í„° ì¤€ë¹„
    common_data = {
        "deviceId": device_id,
        "regDate": now_kst.strftime("%Y-%m-%d %H:%M:%S")
    }
    api_endpoint = ""
    record_data = {}

    if detection_type == 'ppe':
        api_endpoint = "/ppe-records"

        raw = violation_info.get('labels', [])

        try:
            import numpy as _np
            if isinstance(raw, _np.ndarray):
                raw = raw.tolist()
        except Exception:
            pass
        if isinstance(raw, (set, tuple)):
            raw = list(raw)

        labels = [str(x) for x in raw]

        # 1. ë¯¸ì°©ìš© í•­ëª©ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ ìƒì„±
        unworn_items = []
        
        # 2. violation_info['labels']ë¥¼ í™•ì¸í•˜ì—¬ ë¯¸ì°©ìš© í•­ëª© ì¶”ê°€
        if 'Safety Helmet unwear' in labels:
            unworn_items.append("ì•ˆì „ëª¨ ë¯¸ì°©ìš©")
        if 'Safety Hook unwear' in labels:
            unworn_items.append("ì•ˆì „ê³ ë¦¬ ë¯¸ê²°ì°©")
        if 'Safety Belt unwear' in labels:
            unworn_items.append("ì•ˆì „ë²¨íŠ¸ ë¯¸ì°©ìš©")
        if 'Safety Shoes unwear' in labels:
            unworn_items.append("ì•ˆì „í™” ë¯¸ì°©ìš©")
            
        # 3. ë¯¸ì°©ìš© í•­ëª© ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (ì˜ˆ: "ì•ˆì „ëª¨ ë¯¸ì°©ìš©, ì•ˆì „ë²¨íŠ¸ ë¯¸ì°©ìš©")
        violation_details = ", ".join(unworn_items)
        
        # 4. ìµœì¢… content ë¬¸ìì—´ ìƒì„±
        if violation_details:
            # content = f"{now_kst.strftime('%Yë…„ %mì›” %dì¼ %H:%M:%S')}\nì‘ì—…ì(ID: {track_id})ì˜ {violation_details}ì´(ê°€) ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤."
            content = f"ì‘ì—…ì(ID: {track_id})ì˜ {violation_details}ì´(ê°€) ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤."
        else:
            # ì˜ˆì™¸ ìƒí™©ì˜ ê²½ìš°, ë‹¤ìŒ ë¬¸ì¥ì„ ì¶œë ¥í•¨
            # content = f"{now_kst.strftime('%Yë…„ %mì›” %dì¼ %H:%M:%S')}\nì‘ì—…ì(ID: {track_id})ì˜ ì•ˆì „ì¥ë¹„ ë¯¸ì°©ìš©ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤."
            content = f"ì‘ì—…ì(ID: {track_id})ì˜ ì•ˆì „ì¥ë¹„ ë¯¸ì°©ìš©ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤."

        record_data = {
            "deviceId": int(common_data["deviceId"]),
            "originalImg": original_img_url,
            "detectImg": detect_img_url,
            "content": content,
            "helmetOff": 1 if 'Safety Helmet unwear' in labels else 0,
            "hookOff": 1 if 'Safety Hook unwear' in labels else 0,
            "beltOff": 1 if 'Safety Belt unwear' in labels else 0,
            "shoesOff": 1 if 'Safety Shoes unwear' in labels else 0,
            "regDate": common_data["regDate"]
        }

    elif detection_type == 'acc':
        api_endpoint = "/acc-records"
        # content = f"{now_kst.strftime('%Yë…„ %mì›” %dì¼ %H:%M:%S')}\nì‚¬ê³  ìƒí™©ìœ¼ë¡œ ì˜ì‹¬ë˜ëŠ” ì‘ì—…ì(ID: {track_id})ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤."
        content = f"ì‘ì—…ì(ID: {track_id})ì˜ ì‚¬ê³  ìƒí™©(ë‚™ìƒ)ìœ¼ë¡œ ì˜ì‹¬ë˜ëŠ” ìƒí™©ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤."
        record_data = {
            "deviceId": int(common_data["deviceId"]),
            "originalImg": original_img_url,
            "detectImg": detect_img_url,
            "content": content,
            "regDate": common_data["regDate"]
        }

    elif detection_type == 'he':
        api_endpoint = "/he-records"
        heType = violation_info.get('heType')
        heNumber = violation_info.get('heNumber')
        access = violation_info.get('access')
       
        record_data = {
            "deviceId": int(common_data["deviceId"]),
            "heType": heType,
            "heNumber": heNumber,
            "access": access,
            "regDate": common_data["regDate"],
            "originalImg": original_img_url,
            "detectImg": detect_img_url,
        }

    # API í˜¸ì¶œ   
    if api_endpoint:
        try:
            full_api_url = SPRING_BOOT_API_URL + api_endpoint

            payload = _to_py(record_data)                       # numpy ì œê±°
            if 'deviceId' in payload:
                payload['deviceId'] = int(payload['deviceId'])  # INT ê°•ì œ

            headers = {'Content-Type': 'application/json; charset=UTF-8'}
            response = requests.post(full_api_url, json=payload, headers=headers)

            print(f"[POST] {full_api_url} -> {response.status_code}")
            print(f"[REQ ] {payload}")
            print(f"[RESP] {response.text}")

            if 200 <= response.status_code < 300:
                print(f"âœ… {detection_type} ê¸°ë¡ ì „ì†¡ ì„±ê³µ!")
                socketio.emit('new_record_added', {'type': detection_type})
            else:
                print(f"âŒ {detection_type} ê¸°ë¡ ì „ì†¡ ì‹¤íŒ¨: {response.status_code}")
        except Exception as e:
            print(f"API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        finally:
            # ë„¤íŠ¸ì›Œí¬ I/O í›„ ì–‘ë³´
            try:
                eventlet.sleep(0)
            except Exception:
                pass


# =================================================================
# 6. Spring Boot API ì—°ë™ ë° Object Storage ì—…ë¡œë“œ í•¨ìˆ˜
# =================================================================

# [ ì •ë¦¬ ë£¨í”„ í•¨ìˆ˜ ]
def prune_caches_loop():
    while True:
        try:
            now = time.time()

            # 1) ê¸°ë¡ ì¿¨ë‹¤ìš´ ë§µ 
            # last_recorded_times: key -> ts
            stale = [k for k, ts in last_recorded_times.items() if now - ts > COOLDOWN_TTL_SEC]
            for k in stale:
                last_recorded_times.pop(k, None)

            # 2) ì‚¬ê³  ì¶”ì  ìƒíƒœ 
            # fall_status: tid -> {count, ts}
            stale_tids = [tid for tid, v in fall_status.items() if now - v.get('ts', now) > TRACK_TTL_SEC]
            for tid in stale_tids:
                fall_status.pop(tid, None)

            # 3) ì¤‘ì¥ë¹„ ë°©í–¥ íŒë‹¨ íˆìŠ¤í† ë¦¬ 
            # heavy_equipment_history: tid -> {..., ts}
            stale_tids = [tid for tid, v in heavy_equipment_history.items() if now - v.get('ts', now) > TRACK_TTL_SEC]
            for tid in stale_tids:
                heavy_equipment_history.pop(tid, None)

            # 4) ì¤‘ì¥ë¹„ OCR ì¬ì‹œë„ íšŸìˆ˜ 
            # heavy_equipment_ocr_attempts, ì˜¤ë˜ ì•ˆ ë³´ì¸ íŠ¸ë™ ì œê±°
            stale_tids = [tid for tid, last in he_last_seen.items() if now - last > TRACK_TTL_SEC]
            for tid in stale_tids:
                he_last_seen.pop(tid, None)
                heavy_equipment_ocr_attempts.pop(tid, None)

            # 5) í”„ë ˆì„ ìŠ¤ë¡œí‹€ ë§µ 
            # last_frame_ts: (sid, deviceId) -> ts
            stale_keys = [k for k, ts in last_frame_ts.items() if now - ts > FRAME_TS_TTL_SEC]
            for k in stale_keys:
                last_frame_ts.pop(k, None)

            # 6) ì²˜ë¦¬ì¤‘ ì„¸íŠ¸(processing_inflight)
            # í‚¤ê°€ ë‚¨ëŠ” ê²½ìš°ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ê°•ì œ ì •ë¦¬
            inflight_stale = {k for k in list(processing_inflight) if now - last_frame_ts.get(k, 0) > 5}
            for k in inflight_stale:
                processing_inflight.discard(k)

        except Exception as e:
            print(f"[cleanup] error: {e}")
        finally:
            eventlet.sleep(CLEANUP_INTERVAL_SEC)



# =================================================================
# 7. ì„œë²„ ì‹¤í–‰
# =================================================================

@app.route("/ping", methods=["GET"])
def ping():
    return "ok", 200

@app.route("/healthz", methods=["GET"])
def healthz():
    return jsonify({"status": "ok"}), 200

if __name__ == '__main__':
    print("AI ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤ (í¬íŠ¸ 5000)...")
    socketio.start_background_task(prune_caches_loop)
    socketio.run(app, host='0.0.0.0', port=5000, use_reloader=False, debug=True)
